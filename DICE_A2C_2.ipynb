{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import time\n",
    "from numba import njit,guvectorize,float64\n",
    "import scipy.optimize as opt\n",
    "from matplotlib import pyplot as plt\n",
    "import math\n",
    "#Set\n",
    "t = np.arange(1, 101)\n",
    "NT = len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "fosslim = 6000 # Maximum cumulative extraction fossil fuels (GtC); denoted by CCum\n",
    "tstep  = 5 # Years per Period\n",
    "ifopt  = 0 # Indicator where optimized is 1 and base is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preferences\n",
    "\n",
    "elasmu = 1.45 #  Elasticity of marginal utility of consumption\n",
    "prstp = 0.015 #   Initial rate of social time preference per year \n",
    "\n",
    "#** Population and technology\n",
    "gama  = 0.300 #   Capital elasticity in production function         /.300 /\n",
    "pop0  = 7403   # Initial world population 2015 (millions)          /7403 /\n",
    "popadj = 0.134 #  Growth rate to calibrate to 2050 pop projection  /0.134/\n",
    "popasym = 11500 # Asymptotic population (millions)                 /11500/\n",
    "dk  = 0.100 #     Depreciation rate on capital (per year)           /.100 /\n",
    "q0  = 105.5 #     Initial world gross output 2015 (trill 2010 USD) /105.5/\n",
    "k0  = 223 #     Initial capital value 2015 (trill 2010 USD)        /223  /\n",
    "a0  = 5.115 #     Initial level of total factor productivity       /5.115/\n",
    "ga0  = 0.076 #    Initial growth rate for TFP per 5 years          /0.076/\n",
    "dela  = 0.005 #   Decline rate of TFP per 5 years                  /0.005/\n",
    "\n",
    "#** Emissions parameters\n",
    "gsigma1  = -0.0152 # Initial growth of sigma (per year)            /-0.0152/\n",
    "dsig  = -0.001 #   Decline rate of decarbonization (per period)    /-0.001 /\n",
    "eland0 = 2.6 #  Carbon emissions from land 2015 (GtCO2 per year)   / 2.6   /\n",
    "deland = 0.115 # Decline rate of land emissions (per period)        / .115  /\n",
    "e0 = 35.85 #    Industrial emissions 2015 (GtCO2 per year)       /35.85  /\n",
    "miu0  = 0.03 #   Initial emissions control rate for base case 2015  /.03    /\n",
    "\n",
    "#** Carbon cycle\n",
    "#* Initial Conditions\n",
    "mat0 = 851 #  Initial Concentration in atmosphere 2015 (GtC)       /851  /\n",
    "mu0  = 460 #  Initial Concentration in upper strata 2015 (GtC)     /460  /\n",
    "ml0  = 1740 #  Initial Concentration in lower strata 2015 (GtC)    /1740 /\n",
    "mateq = 588 # mateq Equilibrium concentration atmosphere  (GtC)    /588  /\n",
    "mueq  = 360 # mueq Equilibrium concentration in upper strata (GtC) /360  /\n",
    "mleq = 1720 # mleq Equilibrium concentration in lower strata (GtC) /1720 /\n",
    "\n",
    "#* Flow paramaters, denoted by Phi_ij in the model\n",
    "b12  = 0.12 #    Carbon cycle transition matrix                     /.12  /\n",
    "b23  = 0.007 #   Carbon cycle transition matrix                    /0.007/\n",
    "#* These are for declaration and are defined later\n",
    "b11  = None   # Carbon cycle transition matrix\n",
    "b21  = None  # Carbon cycle transition matrix\n",
    "b22  = None  # Carbon cycle transition matrix\n",
    "b32  = None  # Carbon cycle transition matrix\n",
    "b33  = None  # Carbon cycle transition matrix\n",
    "sig0  = None  # Carbon intensity 2010 (kgCO2 per output 2005 USD 2010)\n",
    "\n",
    "#** Climate model parameters\n",
    "t2xco2  = 3.1 # Equilibrium temp impact (oC per doubling CO2)    / 3.1 /\n",
    "fex0  = 0.5 #   2015 forcings of non-CO2 GHG (Wm-2)              / 0.5 /\n",
    "fex1  = 1.0 #   2100 forcings of non-CO2 GHG (Wm-2)              / 1.0 /\n",
    "tocean0  = 0.0068 # Initial lower stratum temp change (C from 1900) /.0068/\n",
    "tatm0  = 0.85 #  Initial atmospheric temp change (C from 1900)    /0.85/\n",
    "c1  = 0.1005 #     Climate equation coefficient for upper level  /0.1005/\n",
    "c3  = 0.088 #     Transfer coefficient upper to lower stratum    /0.088/\n",
    "c4  = 0.025 #     Transfer coefficient for lower level           /0.025/\n",
    "fco22x  = 3.6813 # eta in the model; Eq.22 : Forcings of equilibrium CO2 doubling (Wm-2)   /3.6813 /\n",
    "\n",
    "#** Climate damage parameters\n",
    "a10  = 0 #     Initial damage intercept                         /0   /\n",
    "a20  = None #     Initial damage quadratic term\n",
    "a1  = 0 #      Damage intercept                                 /0   /\n",
    "a2  = 0.00236 #      Damage quadratic term                     /0.00236/\n",
    "a3  = 2.00 #      Damage exponent                              /2.00   /\n",
    "\n",
    "#** Abatement cost\n",
    "expcost2 = 2.6 # Theta2 in the model, Eq. 10 Exponent of control cost function             / 2.6  /\n",
    "pback  = 550 #   Cost of backstop 2010$ per tCO2 2015          / 550  /\n",
    "gback  = 0.025 #   Initial cost decline backstop cost per period / .025/\n",
    "limmiu  = 1.2 #  Upper limit on control rate after 2150        / 1.2 /\n",
    "tnopol  = 45 #  Period before which no emissions controls base  / 45   /\n",
    "cprice0  = 2 # Initial base carbon price (2010$ per tCO2)      / 2    /\n",
    "gcprice  = 0.02 # Growth rate of base carbon price per year     /.02  /\n",
    "\n",
    "#** Scaling and inessential parameters\n",
    "#* Note that these are unnecessary for the calculations\n",
    "#* They ensure that MU of first period's consumption =1 and PV cons = PV utilty\n",
    "scale1  = 0.0302455265681763 #    Multiplicative scaling coefficient           /0.0302455265681763 /\n",
    "scale2  = -10993.704 #    Additive scaling coefficient       /-10993.704/;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* Parameters for long-run consistency of carbon cycle \n",
    "#(Question)\n",
    "b11 = 1 - b12\n",
    "b21 = b12*mateq/mueq\n",
    "b22 = 1 - b21 - b23\n",
    "b32 = b23*mueq/mleq\n",
    "b33 = 1 - b32\n",
    "\n",
    "#* Further definitions of parameters\n",
    "a20 = a2\n",
    "sig0 = e0/(q0*(1-miu0)) #From Eq. 14\n",
    "lam = fco22x/ t2xco2 #From Eq. 25\n",
    "\n",
    "l = np.zeros(NT)\n",
    "l[0] = pop0 #Labor force\n",
    "al = np.zeros(NT) \n",
    "al[0] = a0\n",
    "gsig = np.zeros(NT) \n",
    "gsig[0] = gsigma1\n",
    "sigma = np.zeros(NT)\n",
    "sigma[0]= sig0\n",
    "ga = ga0 * np.exp(-dela*5*(t-1)) #TFP growth rate dynamics, Eq. 7\n",
    "pbacktime = pback * (1-gback)**(t-1) #Backstop price\n",
    "etree = eland0*(1-deland)**(t-1) #Emissions from deforestration\n",
    "rr = 1/((1+prstp)**(tstep*(t-1))) #Eq. 3\n",
    "#The following three equations define the exogenous radiative forcing; used in Eq. 23  \n",
    "forcoth = np.full(NT,fex0)\n",
    "forcoth[0:18] = forcoth[0:18] + (1/17)*(fex1-fex0)*(t[0:18]-1)\n",
    "forcoth[18:NT] = forcoth[18:NT] + (fex1-fex0)\n",
    "optlrsav = (dk + .004)/(dk + .004*elasmu + prstp)*gama #Optimal long-run savings rate used for transversality (Question)\n",
    "cost1 = np.zeros(NT)\n",
    "cumetree = np.zeros(NT)\n",
    "cumetree[0] = 100\n",
    "cpricebase = cprice0*(1+gcprice)**(5*(t-1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit('(float64[:], int32)')\n",
    "def InitializeLabor(il,iNT):\n",
    "    for i in range(1,iNT):\n",
    "        il[i] = il[i-1]*(popasym / il[i-1])**popadj\n",
    "\n",
    "@njit('(float64[:], int32)')        \n",
    "def InitializeTFP(ial,iNT):\n",
    "    for i in range(1,iNT):\n",
    "        ial[i] = ial[i-1]/(1-ga[i-1])\n",
    "        \n",
    "@njit('(float64[:], int32)')        \n",
    "def InitializeGrowthSigma(igsig,iNT):\n",
    "    for i in range(1,iNT):\n",
    "        igsig[i] = igsig[i-1]*((1+dsig)**tstep)\n",
    "        \n",
    "@njit('(float64[:], float64[:],float64[:],int32)')        \n",
    "def InitializeSigma(isigma,igsig,icost1,iNT):\n",
    "    for i in range(1,iNT):\n",
    "        isigma[i] =  isigma[i-1] * np.exp(igsig[i-1] * tstep)\n",
    "        icost1[i] = pbacktime[i] * isigma[i]  / expcost2 /1000\n",
    "        \n",
    "@njit('(float64[:], int32)')        \n",
    "def InitializeCarbonTree(icumetree,iNT):\n",
    "    for i in range(1,iNT):\n",
    "        icumetree[i] = icumetree[i-1] + etree[i-1]*(5/3.666)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Functions of the model\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "First: Functions related to emissions of carbon and weather damages\n",
    "\"\"\"\n",
    "\n",
    "# Retuns the total carbon emissions; Eq. 18\n",
    "@njit('float64(float64[:],int32)') \n",
    "def fE(iEIND,index):\n",
    "    return iEIND[index] + etree[index]\n",
    "\n",
    "#Eq.14: Determines the emission of carbon by industry EIND\n",
    "@njit('float64(float64[:],float64[:],float64[:],int32)') \n",
    "def fEIND(iYGROSS, iMIU, isigma,index):\n",
    "    return isigma[index] * iYGROSS[index] * (1 - iMIU[index])\n",
    "\n",
    "#Cumulative industrial emission of carbon\n",
    "@njit('float64(float64[:],float64[:],int32)') \n",
    "def fCCA(iCCA,iEIND,index):\n",
    "    return iCCA[index-1] + iEIND[index-1] * 5 / 3.666\n",
    "\n",
    "#Cumulative total carbon emission\n",
    "@njit('float64(float64[:],float64[:],int32)')\n",
    "def fCCATOT(iCCA,icumetree,index):\n",
    "    return iCCA[index] + icumetree[index]\n",
    "\n",
    "#Eq. 22: the dynamics of the radiative forcing\n",
    "@njit('float64(float64[:],int32)')\n",
    "def fFORC(iMAT,index):\n",
    "    return fco22x * np.log(iMAT[index]/588.000)/np.log(2) + forcoth[index]\n",
    "\n",
    "# Dynamics of Omega; Eq.9\n",
    "@njit('float64(float64[:],int32)')\n",
    "def fDAMFRAC(iTATM,index):\n",
    "    return a1*iTATM[index] + a2*iTATM[index]**a3\n",
    "\n",
    "#Calculate damages as a function of Gross industrial production; Eq.8 \n",
    "@njit('float64(float64[:],float64[:],int32)')\n",
    "def fDAMAGES(iYGROSS,iDAMFRAC,index):\n",
    "    return iYGROSS[index] * iDAMFRAC[index]\n",
    "\n",
    "#Dynamics of Lambda; Eq. 10 - cost of the reudction of carbon emission (Abatement cost)\n",
    "@njit('float64(float64[:],float64[:],float64[:],int32)') \n",
    "def fABATECOST(iYGROSS,iMIU,icost1,index):\n",
    "    return iYGROSS[index] * icost1[index] * iMIU[index]**expcost2\n",
    "\n",
    "#Marginal Abatement cost\n",
    "@njit('float64(float64[:],int32)')\n",
    "def fMCABATE(iMIU,index):\n",
    "    return pbacktime[index] * iMIU[index]**(expcost2-1)\n",
    "\n",
    "#Price of carbon reduction\n",
    "@njit('float64(float64[:],int32)')\n",
    "def fCPRICE(iMIU,index):\n",
    "    return pbacktime[index] * (iMIU[index])**(expcost2-1)\n",
    "\n",
    "#Eq. 19: Dynamics of the carbon concentration in the atmosphere\n",
    "@njit('float64(float64[:],float64[:],float64[:],int32)') \n",
    "def fMAT(iMAT,iMU,iE,index):\n",
    "    if(index == 0):\n",
    "        return mat0\n",
    "    else:\n",
    "        return iMAT[index-1]*b11 + iMU[index-1]*b21 + iE[index-1] * 5 / 3.666\n",
    "\n",
    "#Eq. 21: Dynamics of the carbon concentration in the ocean LOW level\n",
    "@njit('float64(float64[:],float64[:],int32)') \n",
    "def fML(iML,iMU,index):\n",
    "    if(index == 0):\n",
    "        return ml0\n",
    "    else:\n",
    "        return iML[index-1] * b33  + iMU[index-1] * b23\n",
    "\n",
    "#Eq. 20: Dynamics of the carbon concentration in the ocean UP level\n",
    "@njit('float64(float64[:],float64[:],float64[:],int32)') \n",
    "def fMU(iMAT,iMU,iML,index):\n",
    "    if(index == 0):\n",
    "        return mu0\n",
    "    else:\n",
    "        return iMAT[index-1]*b12 + iMU[index-1]*b22 + iML[index-1]*b32\n",
    "\n",
    "#Eq. 23: Dynamics of the atmospheric temperature\n",
    "@njit('float64(float64[:],float64[:],float64[:],int32)') \n",
    "def fTATM(iTATM,iFORC,iTOCEAN,index):\n",
    "    if(index == 0):\n",
    "        return tatm0\n",
    "    else:\n",
    "        return iTATM[index-1] + c1 * (iFORC[index] - (fco22x/t2xco2) * iTATM[index-1] - c3 * (iTATM[index-1] - iTOCEAN[index-1]))\n",
    "\n",
    "#Eq. 24: Dynamics of the ocean temperature\n",
    "@njit('float64(float64[:],float64[:],int32)')\n",
    "def fTOCEAN(iTATM,iTOCEAN,index):\n",
    "    if(index == 0):\n",
    "        return tocean0\n",
    "    else:\n",
    "        return iTOCEAN[index-1] + c4 * (iTATM[index-1] - iTOCEAN[index-1])\n",
    "\n",
    "\"\"\"\n",
    "Second: Function related to economic variables\n",
    "\"\"\"\n",
    "\n",
    "#The total production without climate losses denoted previously by YGROSS\n",
    "@njit('float64(float64[:],float64[:],float64[:],int32)')\n",
    "def fYGROSS(ial,il,iK,index):\n",
    "    return ial[index] * ((il[index]/1000)**(1-gama)) * iK[index]**gama\n",
    "\n",
    "#The production under the climate damages cost\n",
    "@njit('float64(float64[:],float64[:],int32)')\n",
    "def fYNET(iYGROSS, iDAMFRAC, index):\n",
    "    return iYGROSS[index] * (1 - iDAMFRAC[index])\n",
    "\n",
    "#Production after abatement cost\n",
    "@njit('float64(float64[:],float64[:],int32)')\n",
    "def fY(iYNET,iABATECOST,index):\n",
    "    return iYNET[index] - iABATECOST[index]\n",
    "\n",
    "#Consumption Eq. 11\n",
    "@njit('float64(float64[:],float64[:],int32)')\n",
    "def fC(iY,iI,index):\n",
    "    return iY[index] - iI[index]\n",
    "\n",
    "#Per capita consumption, Eq. 12\n",
    "@njit('float64(float64[:],float64[:],int32)')\n",
    "def fCPC(iC,il,index):\n",
    "    return 1000 * iC[index] / il[index]\n",
    "\n",
    "#Saving policy: investment\n",
    "@njit('float64(float64[:],float64[:],int32)')\n",
    "def fI(iS,iY,index):\n",
    "    return iS[index] * iY[index] \n",
    "\n",
    "#Capital dynamics Eq. 13\n",
    "@njit('float64(float64[:],float64[:],int32)')\n",
    "def fK(iK,iI,index):\n",
    "    if(index == 0):\n",
    "        return k0\n",
    "    else:\n",
    "        return (1-dk)**tstep * iK[index-1] + tstep * iI[index-1]\n",
    "\n",
    "#Interest rate equation; Eq. 26 added in personal notes\n",
    "@njit('float64(float64[:],int32)')\n",
    "def fRI(iCPC,index):\n",
    "    return (1 + prstp) * (iCPC[index+1]/iCPC[index])**(elasmu/tstep) - 1\n",
    "\n",
    "#Periodic utility: A form of Eq. 2\n",
    "@njit('float64(float64[:],float64[:],int32)')\n",
    "def fCEMUTOTPER(iPERIODU,il,index):\n",
    "    return iPERIODU[index] * il[index] * rr[index]\n",
    "\n",
    "#The term between brackets in Eq. 2\n",
    "@njit('float64(float64[:],float64[:],int32)')\n",
    "def fPERIODU(iC,il,index):\n",
    "    return ((iC[index]*1000/il[index])**(1-elasmu) - 1) / (1 - elasmu) - 1\n",
    "\n",
    "#utility function\n",
    "@guvectorize([(float64[:], float64[:])], '(n), (m)')\n",
    "def fUTILITY(iCEMUTOTPER, resUtility):\n",
    "    resUtility[0] = tstep * scale1 * np.sum(iCEMUTOTPER) + scale2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "In this part we implement the objective function\n",
    "\"\"\"\n",
    "\n",
    "# * Control rate limits\n",
    "MIU_lo = np.full(NT,0.01)\n",
    "MIU_up = np.full(NT,limmiu)\n",
    "MIU_up[0:29] = 1\n",
    "MIU_lo[0] = miu0\n",
    "MIU_up[0] = miu0\n",
    "MIU_lo[MIU_lo==MIU_up] = 0.99999*MIU_lo[MIU_lo==MIU_up]\n",
    "bnds1=[]\n",
    "for i in range(NT):\n",
    "    bnds1.append((MIU_lo[i],MIU_up[i]))\n",
    "# * Control variables\n",
    "lag10 = t > NT - 10\n",
    "S_lo = np.full(NT,1e-1)\n",
    "S_lo[lag10] = optlrsav\n",
    "S_up = np.full(NT,0.9)\n",
    "S_up[lag10] = optlrsav\n",
    "S_lo[S_lo==S_up] = 0.99999*S_lo[S_lo==S_up]\n",
    "bnds2=[]\n",
    "for i in range(NT):\n",
    "    bnds2.append((S_lo[i],S_up[i]))\n",
    "    \n",
    "# Arbitrary starting values for the control variables:\n",
    "S_start = np.full(NT,0.2)\n",
    "S_start[S_start < S_lo] = S_lo[S_start < S_lo]\n",
    "S_start[S_start > S_up] = S_lo[S_start > S_up]\n",
    "MIU_start = 0.99*MIU_up\n",
    "MIU_start[MIU_start < MIU_lo] = MIU_lo[MIU_start < MIU_lo]\n",
    "MIU_start[MIU_start > MIU_up] = MIU_up[MIU_start > MIU_up]\n",
    "\n",
    "K = np.zeros(NT)\n",
    "YGROSS = np.zeros(NT)\n",
    "EIND = np.zeros(NT)\n",
    "E = np.zeros(NT)\n",
    "CCA = np.zeros(NT)\n",
    "CCATOT = np.zeros(NT)\n",
    "MAT = np.zeros(NT)\n",
    "ML = np.zeros(NT)\n",
    "MU = np.zeros(NT)\n",
    "FORC = np.zeros(NT)\n",
    "TATM = np.zeros(NT)\n",
    "TOCEAN = np.zeros(NT)\n",
    "DAMFRAC = np.zeros(NT)\n",
    "DAMAGES = np.zeros(NT)\n",
    "ABATECOST = np.zeros(NT)\n",
    "MCABATE = np.zeros(NT)\n",
    "CPRICE = np.zeros(NT)\n",
    "YNET = np.zeros(NT)\n",
    "Y = np.zeros(NT)\n",
    "I = np.zeros(NT)\n",
    "C = np.zeros(NT)\n",
    "CPC = np.zeros(NT)\n",
    "RI = np.zeros(NT)\n",
    "PERIODU = np.zeros(NT)\n",
    "CEMUTOTPER = np.zeros(NT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The objective function\n",
    "#It returns the utility as scalar\n",
    "def fOBJ(x,sign,iI,iK,ial,il,iYGROSS,isigma,iEIND,iE,iCCA,iCCATOT,icumetree,iMAT,iMU,iML,iFORC,iTATM,iTOCEAN,iDAMFRAC,iDAMAGES,iABATECOST,icost1,iMCABATE,\n",
    "         iCPRICE,iYNET,iY,iC,iCPC,iPERIODU,iCEMUTOTPER,iRI,iNT):\n",
    "    \n",
    "    iMIU = x[0:NT]\n",
    "    iS = x[NT:(2*NT)]\n",
    "    \n",
    "    for i in range(iNT):\n",
    "        iK[i] = fK(iK,iI,i)\n",
    "        iYGROSS[i] = fYGROSS(ial,il,iK,i)\n",
    "        iEIND[i] = fEIND(iYGROSS, iMIU, isigma,i)\n",
    "        iE[i] = fE(iEIND,i)\n",
    "        iCCA[i] = fCCA(iCCA,iEIND,i)\n",
    "        iCCATOT[i] = fCCATOT(iCCA,icumetree,i)\n",
    "        iMAT[i] = fMAT(iMAT,iMU,iE,i)\n",
    "        iML[i] = fML(iML,iMU,i)\n",
    "        iMU[i] = fMU(iMAT,iMU,iML,i)\n",
    "        iFORC[i] = fFORC(iMAT,i)\n",
    "        iTATM[i] = fTATM(iTATM,iFORC,iTOCEAN,i)\n",
    "        iTOCEAN[i] = fTOCEAN(iTATM,iTOCEAN,i)\n",
    "        iDAMFRAC[i] = fDAMFRAC(iTATM,i)\n",
    "        iDAMAGES[i] = fDAMAGES(iYGROSS,iDAMFRAC,i)\n",
    "        iABATECOST[i] = fABATECOST(iYGROSS,iMIU,icost1,i)\n",
    "        iMCABATE[i] = fMCABATE(iMIU,i)\n",
    "        iCPRICE[i] = fCPRICE(iMIU,i)\n",
    "        iYNET[i] = fYNET(iYGROSS, iDAMFRAC, i)\n",
    "        iY[i] = fY(iYNET,iABATECOST,i)\n",
    "        iI[i] = fI(iS,iY,i)\n",
    "        iC[i] = fC(iY,iI,i)\n",
    "        iCPC[i] = fCPC(iC,il,i)\n",
    "        iPERIODU[i] = fPERIODU(iC,il,i)\n",
    "        iCEMUTOTPER[i] = fCEMUTOTPER(iPERIODU,il,i)\n",
    "        iRI = fRI(iCPC,i)\n",
    "        \n",
    "    resUtility = np.zeros(1)\n",
    "    fUTILITY(iCEMUTOTPER, resUtility)\n",
    "    \n",
    "    return sign*resUtility[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "InitializeLabor(l,NT)\n",
    "InitializeTFP(al,NT)\n",
    "InitializeGrowthSigma(gsig,NT)\n",
    "InitializeSigma(sigma,gsig,cost1,NT)\n",
    "InitializeCarbonTree(cumetree,NT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "# Configuration parameters for the whole setup\n",
    "gamma = 0.99  # Discount factor for past rewards\n",
    "max_steps_per_episode = 1000\n",
    "eps = np.finfo(np.float32).eps.item()  # Smallest number such that 1.0 + eps != 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OPTIM(object):\n",
    "    def __init__(self):\n",
    "        super(OPTIM, self).__init__()\n",
    "    \n",
    "    def step(self, action):\n",
    "        global x\n",
    "        \n",
    "        miu = action[0]\n",
    "        s = action[1]\n",
    "        x[steps%100] = miu\n",
    "        x[steps%100+100] = s\n",
    "        #obs_ = np.array([miu,s])\n",
    "        #obs_ = np.array([miu,s,steps+1])\n",
    "        reward = fOBJ(x,1,I,K,al,l,YGROSS,sigma,EIND,E,CCA,CCATOT,cumetree,MAT,MU,ML,FORC,\n",
    "                      TATM,TOCEAN,DAMFRAC,DAMAGES,ABATECOST,cost1,MCABATE,CPRICE,YNET,Y,C,CPC,PERIODU,CEMUTOTPER,RI,NT)\n",
    "        R = reward / 4500\n",
    "        done = bool(R > 1 or miu == np.nan)\n",
    "        #if done == True:\n",
    "            #if reward > 4500:\n",
    "             #   R = 2\n",
    "            #else:\n",
    "            #    R = 1\n",
    "        #else:\n",
    "            #R = 0\n",
    "        #if steps == 99:\n",
    "        #print(R)\n",
    "        return x, R, done\n",
    "        \n",
    "    def reset(self):\n",
    "        return np.concatenate([MIU_start,S_start])\n",
    "        #return np.array([MIU_start[0],S_start[0],0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = OPTIM()\n",
    "num_inputs = 200\n",
    "num_hidden = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = layers.Input(shape=(num_inputs,))\n",
    "common = layers.Dense(num_hidden, activation=\"relu\")(inputs)\n",
    "critic = layers.Dense(1)(common)\n",
    "#4 for 2 miu and 2 sigmas of 2 iid normal distributions\n",
    "norm_par = layers.Dense(4)(common)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[norm_par, critic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: 6.93, reward: 0.15 at episode 1\n",
      "running reward: 9.94, reward: 0.34 at episode 2\n",
      "running reward: 0.47, reward: 0.04 at episode 3\n",
      "running reward: 2.07, reward: 0.17 at episode 4\n",
      "running reward: -10.63, reward: 0.22 at episode 5\n",
      "running reward: -34.63, reward: -0.51 at episode 6\n",
      "running reward: -75.68, reward: -1.16 at episode 7\n",
      "running reward: -121.78, reward: -1.13 at episode 8\n",
      "running reward: -149.42, reward: -0.91 at episode 9\n",
      "running reward: -113.65, reward: 0.55 at episode 10\n",
      "running reward: -74.49, reward: 0.75 at episode 11\n",
      "running reward: -31.09, reward: 0.78 at episode 12\n",
      "running reward: 14.50, reward: 0.95 at episode 13\n",
      "running reward: 54.18, reward: 0.89 at episode 14\n",
      "running reward: 93.63, reward: 0.81 at episode 15\n",
      "running reward: 119.22, reward: 0.27 at episode 16\n",
      "running reward: 49.26, reward: -1.31 at episode 17\n",
      "running reward: -17.65, reward: -1.31 at episode 18\n",
      "running reward: -82.27, reward: -1.34 at episode 19\n",
      "running reward: -143.80, reward: -1.34 at episode 20\n",
      "running reward: -202.27, reward: -1.34 at episode 21\n",
      "running reward: -257.81, reward: -1.34 at episode 22\n",
      "running reward: -310.58, reward: -1.34 at episode 23\n",
      "running reward: -360.71, reward: -1.34 at episode 24\n",
      "running reward: -408.33, reward: -1.34 at episode 25\n",
      "running reward: -453.57, reward: -1.34 at episode 26\n",
      "running reward: -496.55, reward: -1.34 at episode 27\n",
      "running reward: -537.38, reward: -1.34 at episode 28\n",
      "running reward: -576.16, reward: -1.34 at episode 29\n",
      "running reward: -613.01, reward: -1.34 at episode 30\n",
      "running reward: -648.02, reward: -1.34 at episode 31\n",
      "running reward: -681.27, reward: -1.34 at episode 32\n",
      "running reward: -712.87, reward: -1.34 at episode 33\n",
      "running reward: -742.88, reward: -1.34 at episode 34\n",
      "running reward: -771.39, reward: -1.34 at episode 35\n",
      "running reward: -798.48, reward: -1.34 at episode 36\n",
      "running reward: -824.21, reward: -1.34 at episode 37\n",
      "running reward: -848.66, reward: -1.34 at episode 38\n",
      "running reward: -871.88, reward: -1.34 at episode 39\n",
      "running reward: -893.95, reward: -1.34 at episode 40\n",
      "running reward: -914.91, reward: -1.34 at episode 41\n",
      "running reward: -934.82, reward: -1.34 at episode 42\n",
      "running reward: -953.73, reward: -1.34 at episode 43\n",
      "running reward: -971.70, reward: -1.34 at episode 44\n",
      "running reward: -988.78, reward: -1.34 at episode 45\n",
      "running reward: -1004.99, reward: -1.34 at episode 46\n",
      "running reward: -1020.40, reward: -1.34 at episode 47\n",
      "running reward: -1035.04, reward: -1.34 at episode 48\n",
      "running reward: -1048.94, reward: -1.34 at episode 49\n",
      "running reward: -1062.15, reward: -1.34 at episode 50\n",
      "running reward: -1074.70, reward: -1.34 at episode 51\n",
      "running reward: -1086.62, reward: -1.34 at episode 52\n",
      "running reward: -1097.95, reward: -1.34 at episode 53\n",
      "running reward: -1108.71, reward: -1.34 at episode 54\n",
      "running reward: -1118.93, reward: -1.34 at episode 55\n",
      "running reward: -1128.64, reward: -1.34 at episode 56\n",
      "running reward: -1137.87, reward: -1.34 at episode 57\n",
      "running reward: -1146.63, reward: -1.34 at episode 58\n",
      "running reward: -1154.96, reward: -1.34 at episode 59\n",
      "running reward: -1162.86, reward: -1.34 at episode 60\n",
      "running reward: -1170.38, reward: -1.34 at episode 61\n",
      "running reward: -1177.52, reward: -1.34 at episode 62\n",
      "running reward: -1184.30, reward: -1.34 at episode 63\n",
      "running reward: -1190.74, reward: -1.34 at episode 64\n",
      "running reward: -1196.86, reward: -1.34 at episode 65\n",
      "running reward: -1202.67, reward: -1.34 at episode 66\n",
      "running reward: -1208.20, reward: -1.34 at episode 67\n",
      "running reward: -1213.44, reward: -1.34 at episode 68\n",
      "running reward: -1218.43, reward: -1.34 at episode 69\n",
      "running reward: -1223.16, reward: -1.34 at episode 70\n",
      "running reward: -1227.66, reward: -1.34 at episode 71\n",
      "running reward: -1231.94, reward: -1.34 at episode 72\n",
      "running reward: -1236.00, reward: -1.34 at episode 73\n",
      "running reward: -1239.85, reward: -1.34 at episode 74\n",
      "running reward: -1243.52, reward: -1.34 at episode 75\n",
      "running reward: -1247.00, reward: -1.34 at episode 76\n",
      "running reward: -1250.31, reward: -1.34 at episode 77\n",
      "running reward: -1253.45, reward: -1.34 at episode 78\n",
      "running reward: -1256.43, reward: -1.34 at episode 79\n",
      "running reward: -1259.27, reward: -1.34 at episode 80\n",
      "running reward: -1261.96, reward: -1.34 at episode 81\n",
      "running reward: -1264.52, reward: -1.34 at episode 82\n",
      "running reward: -1266.95, reward: -1.34 at episode 83\n",
      "running reward: -1269.26, reward: -1.34 at episode 84\n",
      "running reward: -1271.45, reward: -1.34 at episode 85\n",
      "running reward: -1273.54, reward: -1.34 at episode 86\n",
      "running reward: -1275.52, reward: -1.34 at episode 87\n",
      "running reward: -1277.40, reward: -1.34 at episode 88\n",
      "running reward: -1279.19, reward: -1.34 at episode 89\n",
      "running reward: -1280.88, reward: -1.34 at episode 90\n",
      "running reward: -1282.50, reward: -1.34 at episode 91\n",
      "running reward: -1284.03, reward: -1.34 at episode 92\n",
      "running reward: -1285.48, reward: -1.34 at episode 93\n",
      "running reward: -1286.87, reward: -1.34 at episode 94\n",
      "running reward: -1288.18, reward: -1.34 at episode 95\n",
      "running reward: -1289.43, reward: -1.34 at episode 96\n",
      "running reward: -1290.61, reward: -1.34 at episode 97\n",
      "running reward: -1291.74, reward: -1.34 at episode 98\n",
      "running reward: -1292.81, reward: -1.34 at episode 99\n",
      "running reward: -1293.83, reward: -1.34 at episode 100\n",
      "running reward: -1294.79, reward: -1.34 at episode 101\n",
      "running reward: -1295.71, reward: -1.34 at episode 102\n",
      "running reward: -1296.58, reward: -1.34 at episode 103\n",
      "running reward: -1297.41, reward: -1.34 at episode 104\n",
      "running reward: -1298.20, reward: -1.34 at episode 105\n",
      "running reward: -1298.94, reward: -1.34 at episode 106\n",
      "running reward: -1299.65, reward: -1.34 at episode 107\n",
      "running reward: -1300.33, reward: -1.34 at episode 108\n",
      "running reward: -1300.97, reward: -1.34 at episode 109\n",
      "running reward: -1301.58, reward: -1.34 at episode 110\n",
      "running reward: -1302.15, reward: -1.34 at episode 111\n",
      "running reward: -1302.70, reward: -1.34 at episode 112\n",
      "running reward: -1303.23, reward: -1.34 at episode 113\n",
      "running reward: -1303.72, reward: -1.34 at episode 114\n",
      "running reward: -1304.19, reward: -1.34 at episode 115\n",
      "running reward: -1304.64, reward: -1.34 at episode 116\n",
      "running reward: -1305.06, reward: -1.34 at episode 117\n",
      "running reward: -1305.47, reward: -1.34 at episode 118\n",
      "running reward: -1305.85, reward: -1.34 at episode 119\n",
      "running reward: -1306.22, reward: -1.34 at episode 120\n",
      "running reward: -1306.56, reward: -1.34 at episode 121\n",
      "running reward: -1306.89, reward: -1.34 at episode 122\n",
      "running reward: -1307.20, reward: -1.34 at episode 123\n",
      "running reward: -1307.50, reward: -1.34 at episode 124\n",
      "running reward: -1307.78, reward: -1.34 at episode 125\n",
      "running reward: -1308.05, reward: -1.34 at episode 126\n",
      "running reward: -1308.30, reward: -1.34 at episode 127\n",
      "running reward: -1308.55, reward: -1.34 at episode 128\n",
      "running reward: -1308.78, reward: -1.34 at episode 129\n",
      "running reward: -1308.99, reward: -1.34 at episode 130\n",
      "running reward: -1309.20, reward: -1.34 at episode 131\n",
      "running reward: -1309.40, reward: -1.34 at episode 132\n",
      "running reward: -1309.59, reward: -1.34 at episode 133\n",
      "running reward: -1309.76, reward: -1.34 at episode 134\n",
      "running reward: -1309.93, reward: -1.34 at episode 135\n",
      "running reward: -1310.09, reward: -1.34 at episode 136\n",
      "running reward: -1310.24, reward: -1.34 at episode 137\n",
      "running reward: -1310.39, reward: -1.34 at episode 138\n",
      "running reward: -1310.53, reward: -1.34 at episode 139\n",
      "running reward: -1310.66, reward: -1.34 at episode 140\n",
      "running reward: -1310.78, reward: -1.34 at episode 141\n",
      "running reward: -1310.90, reward: -1.34 at episode 142\n",
      "running reward: -1311.01, reward: -1.34 at episode 143\n",
      "running reward: -1311.12, reward: -1.34 at episode 144\n",
      "running reward: -1311.22, reward: -1.34 at episode 145\n",
      "running reward: -1311.31, reward: -1.34 at episode 146\n",
      "running reward: -1311.41, reward: -1.34 at episode 147\n",
      "running reward: -1311.49, reward: -1.34 at episode 148\n",
      "running reward: -1311.57, reward: -1.34 at episode 149\n",
      "running reward: -1311.65, reward: -1.34 at episode 150\n",
      "running reward: -1311.73, reward: -1.34 at episode 151\n",
      "running reward: -1311.80, reward: -1.34 at episode 152\n",
      "running reward: -1311.87, reward: -1.34 at episode 153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running reward: -1311.93, reward: -1.34 at episode 154\n",
      "running reward: -1311.99, reward: -1.34 at episode 155\n",
      "running reward: -1312.05, reward: -1.34 at episode 156\n",
      "running reward: -1312.10, reward: -1.34 at episode 157\n",
      "running reward: -1312.15, reward: -1.34 at episode 158\n",
      "running reward: -1312.20, reward: -1.34 at episode 159\n",
      "running reward: -1312.25, reward: -1.34 at episode 160\n",
      "running reward: -1312.29, reward: -1.34 at episode 161\n",
      "running reward: -1312.34, reward: -1.34 at episode 162\n",
      "running reward: -1312.38, reward: -1.34 at episode 163\n",
      "running reward: -1312.41, reward: -1.34 at episode 164\n",
      "running reward: -1312.45, reward: -1.34 at episode 165\n",
      "running reward: -1312.49, reward: -1.34 at episode 166\n",
      "running reward: -1312.52, reward: -1.34 at episode 167\n",
      "running reward: -1312.55, reward: -1.34 at episode 168\n",
      "running reward: -1312.58, reward: -1.34 at episode 169\n",
      "running reward: -1312.61, reward: -1.34 at episode 170\n",
      "running reward: -1312.63, reward: -1.34 at episode 171\n",
      "running reward: -1312.66, reward: -1.34 at episode 172\n",
      "running reward: -1312.68, reward: -1.34 at episode 173\n",
      "running reward: -1312.71, reward: -1.34 at episode 174\n",
      "running reward: -1312.73, reward: -1.34 at episode 175\n",
      "running reward: -1312.75, reward: -1.34 at episode 176\n",
      "running reward: -1312.77, reward: -1.34 at episode 177\n",
      "running reward: -1312.79, reward: -1.34 at episode 178\n",
      "running reward: -1312.80, reward: -1.34 at episode 179\n",
      "running reward: -1312.82, reward: -1.34 at episode 180\n",
      "running reward: -1312.84, reward: -1.34 at episode 181\n",
      "running reward: -1312.85, reward: -1.34 at episode 182\n",
      "running reward: -1312.87, reward: -1.34 at episode 183\n",
      "running reward: -1312.88, reward: -1.34 at episode 184\n",
      "running reward: -1312.89, reward: -1.34 at episode 185\n",
      "running reward: -1312.90, reward: -1.34 at episode 186\n",
      "running reward: -1312.92, reward: -1.34 at episode 187\n",
      "running reward: -1312.93, reward: -1.34 at episode 188\n",
      "running reward: -1312.94, reward: -1.34 at episode 189\n",
      "running reward: -1312.95, reward: -1.34 at episode 190\n",
      "running reward: -1312.96, reward: -1.34 at episode 191\n",
      "running reward: -1312.97, reward: -1.34 at episode 192\n",
      "running reward: -1312.98, reward: -1.34 at episode 193\n",
      "running reward: -1312.98, reward: -1.34 at episode 194\n",
      "running reward: -1312.99, reward: -1.34 at episode 195\n",
      "running reward: -1313.00, reward: -1.34 at episode 196\n",
      "running reward: -1313.01, reward: -1.34 at episode 197\n",
      "running reward: -1313.01, reward: -1.34 at episode 198\n",
      "running reward: -1313.02, reward: -1.34 at episode 199\n",
      "running reward: -1313.02, reward: -1.34 at episode 200\n",
      "running reward: -1313.03, reward: -1.34 at episode 201\n",
      "running reward: -1313.04, reward: -1.34 at episode 202\n",
      "running reward: -1313.04, reward: -1.34 at episode 203\n",
      "running reward: -1313.05, reward: -1.34 at episode 204\n",
      "running reward: -1313.05, reward: -1.34 at episode 205\n",
      "running reward: -1313.06, reward: -1.34 at episode 206\n",
      "running reward: -1313.06, reward: -1.34 at episode 207\n",
      "running reward: -1313.06, reward: -1.34 at episode 208\n",
      "running reward: -1313.07, reward: -1.34 at episode 209\n",
      "running reward: -1313.07, reward: -1.34 at episode 210\n",
      "running reward: -1313.07, reward: -1.34 at episode 211\n",
      "running reward: -1313.08, reward: -1.34 at episode 212\n",
      "running reward: -1313.08, reward: -1.34 at episode 213\n",
      "running reward: -1313.08, reward: -1.34 at episode 214\n",
      "running reward: -1313.09, reward: -1.34 at episode 215\n",
      "running reward: -1313.09, reward: -1.34 at episode 216\n",
      "running reward: -1313.09, reward: -1.34 at episode 217\n",
      "running reward: -1313.09, reward: -1.34 at episode 218\n",
      "running reward: -1313.10, reward: -1.34 at episode 219\n",
      "running reward: -1313.10, reward: -1.34 at episode 220\n",
      "running reward: -1313.10, reward: -1.34 at episode 221\n",
      "running reward: -1313.10, reward: -1.34 at episode 222\n",
      "running reward: -1313.10, reward: -1.34 at episode 223\n",
      "running reward: -1313.11, reward: -1.34 at episode 224\n",
      "running reward: -1313.11, reward: -1.34 at episode 225\n",
      "running reward: -1313.11, reward: -1.34 at episode 226\n",
      "running reward: -1313.11, reward: -1.34 at episode 227\n",
      "running reward: -1313.11, reward: -1.34 at episode 228\n",
      "running reward: -1313.11, reward: -1.34 at episode 229\n",
      "running reward: -1313.11, reward: -1.34 at episode 230\n",
      "running reward: -1313.12, reward: -1.34 at episode 231\n",
      "running reward: -1313.12, reward: -1.34 at episode 232\n",
      "running reward: -1313.12, reward: -1.34 at episode 233\n",
      "running reward: -1313.12, reward: -1.34 at episode 234\n",
      "running reward: -1313.12, reward: -1.34 at episode 235\n",
      "running reward: -1313.12, reward: -1.34 at episode 236\n",
      "running reward: -1313.12, reward: -1.34 at episode 237\n",
      "running reward: -1313.12, reward: -1.34 at episode 238\n",
      "running reward: -1313.12, reward: -1.34 at episode 239\n",
      "running reward: -1313.12, reward: -1.34 at episode 240\n",
      "running reward: -1313.13, reward: -1.34 at episode 241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d6e0b9d96885>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     95\u001b[0m         \u001b[1;31m# Backpropagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mloss_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mactor_losses\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcritic_losses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 97\u001b[1;33m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1071\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1073\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36m_SquaredDifferenceGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m   1574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1575\u001b[0m   (sx, rx, must_reduce_x), (sy, ry, must_reduce_y) = (\n\u001b[1;32m-> 1576\u001b[1;33m       SmartBroadcastGradientArgs(x, y, grad))\n\u001b[0m\u001b[0;32m   1577\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1578\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mskip_input_indices\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mskip_input_indices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_grad.py\u001b[0m in \u001b[0;36mSmartBroadcastGradientArgs\u001b[1;34m(x, y, grad)\u001b[0m\n\u001b[0;32m     92\u001b[0m       \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m       and isinstance(grad, ops.Tensor)):\n\u001b[1;32m---> 94\u001b[1;33m     \u001b[0msx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m     \u001b[0msy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[0mrx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mry\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbroadcast_gradient_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[1;34m(input, name, out_type)\u001b[0m\n\u001b[0;32m    626\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mof\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mout_type\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m   \"\"\"\n\u001b[1;32m--> 628\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mshape_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mshape_internal\u001b[1;34m(input, name, optimize, out_type)\u001b[0m\n\u001b[0;32m    654\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moptimize\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_fully_defined\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    655\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 656\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36mshape\u001b[1;34m(input, out_type, name)\u001b[0m\n\u001b[0;32m   9018\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m   9019\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_context_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Shape\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 9020\u001b[1;33m         tld.op_callbacks, input, \"out_type\", out_type)\n\u001b[0m\u001b[0;32m   9021\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   9022\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=100,\n",
    "    decay_rate=0.8)\n",
    "optimizer = keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "huber_loss = keras.losses.Huber()\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "Reward = []\n",
    "epi = []\n",
    "\n",
    "while True:  # Run until solved\n",
    "    state = env.reset()\n",
    "    x = env.reset()\n",
    "    episode_reward = 0\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        steps = 0\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "            # Predict action probabilities and estimated future rewards\n",
    "            # from environment state\n",
    "            norm_par, critic_value = model(state)\n",
    "            norm_dist = tfd.MultivariateNormalDiag(loc=[norm_par[0, 0], norm_par[0, 1]], \n",
    "                                                   scale_diag=[tf.nn.softplus(norm_par[0, 2])+1e-5, \n",
    "                                                               tf.nn.softplus(norm_par[0, 3])+1e-5])\n",
    "            \n",
    "            # Sample action from action probability distribution\n",
    "            action = tf.squeeze(np.absolute(norm_dist.sample(1)), axis=0)\n",
    "            action = tf.clip_by_value(tf.expand_dims(action, 1), tf.constant([MIU_lo[steps%100], S_lo[steps%100]], shape = [2,1], dtype=tf.float32),\n",
    "                                      tf.constant([MIU_up[steps%100], S_up[steps%100]], shape = [2,1], dtype=tf.float32))                \n",
    "                \n",
    "            tflog = tf.math.log(norm_dist.prob(tf.squeeze(action))+1)\n",
    "                \n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "            action_probs_history.append(tflog)\n",
    "\n",
    "            # Apply the sampled action in our environment\n",
    "            state, reward, done = env.step(action)\n",
    "            rewards_history.append(reward)\n",
    "            episode_reward += reward\n",
    "            \n",
    "            #if timestep == max_steps_per_episode-1:\n",
    "                #print(\"episode_count\", episode_count)\n",
    "                #print(\"norm_par\", norm_par)\n",
    "                #print(\"critic_value\", critic_value)\n",
    "                ##print(\"action\", action)\n",
    "                #print(\"tflog\", tflog)\n",
    "            #print(state)\n",
    "            \n",
    "            steps += 1\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            if timestep == max_steps_per_episode-1:\n",
    "                break\n",
    "\n",
    "        # Update running reward to check condition for solving\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # Calculate expected value from rewards\n",
    "        # - At each timestep what was the total reward received after that timestep\n",
    "        # - Rewards in the past are discounted by multiplying them with gamma\n",
    "        # - These are the labels for our critic\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        # Normalize\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # Calculating loss values to update our network\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "            # The critic must be updated so that it predicts a better estimate of\n",
    "            # the future rewards.\n",
    "            critic_losses.append(\n",
    "                huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "            )\n",
    "\n",
    "        # Backpropagation\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # Clear the loss and reward history\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    # Log details\n",
    "    episode_count += 1\n",
    "    template = \"running reward: {:.2f}, reward: {:.2f} at episode {}\"\n",
    "    print(template.format(running_reward, reward, episode_count))\n",
    "    \n",
    "    Reward.append(reward)\n",
    "    epi.append(episode_count)\n",
    "    \n",
    "    if reward > 1:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break\n",
    "        \n",
    "    if episode_count == 1000:  \n",
    "        print(\"Reached episode {}!\".format(episode_count))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(epi, Reward)\n",
    "\n",
    "ax.set(xlabel='Episode', ylabel='Reward',\n",
    "       title='Reward vs. Episode')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
